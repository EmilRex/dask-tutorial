{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c22a243-93a0-473c-95ba-a718baa2edf0",
   "metadata": {},
   "source": [
    "# Scaling, Performance, and Memory\n",
    "\n",
    "In this notebook we will work with a multi-machine cluster operating in the cloud.  We will do performance tuning on a workflow that enables interactie visualization, and learn about how to measure and improve performance in a distributed context.  We'll make some pretty images too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001ca7e-4d5d-4d90-b456-7c21429d8060",
   "metadata": {},
   "source": [
    "## Request Dask Cluster\n",
    "\n",
    "There are many services to create Dask clusters in the cloud.  Today we'll use [Coiled](https://coiled.io).\n",
    "\n",
    "This should take a couple minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e571e7-26cc-4aed-b946-3c1e7fa412e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !coiled login --token TOKEN --account coiled-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4c47f-0556-47e2-b9d1-c80b54a59041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    n_workers=10,\n",
    "    package_sync=True,\n",
    "    # Note: package_sync is an experimental new feature for Coiled. If something breaks, \n",
    "    # you may want to replace that line with this one, specifying a pre-built software\n",
    "    # environment:\n",
    "    # software=\"nyc-tutorial-2022\"\n",
    "    scheduler_port=443\n",
    ")\n",
    "\n",
    "#\n",
    "\n",
    "from dask.distributed import Client, wait\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea17dca-18fc-433b-89e5-748faa599ba7",
   "metadata": {},
   "source": [
    "## Large Scale GIS Visualization\n",
    "\n",
    "For our application we'll visualize the taxi pickup locations in the classic NYC Taxi dataset.  \n",
    "\n",
    "This data is available to us in Parquet format on S3.  Let's take a brief look at the first few rows and see how many rows we have in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc4687b-2eca-4e5f-8799-94113e02959c",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33582164-1ca1-4685-beb0-03502334baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in one year of NYC Taxi data\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet(\n",
    "    \"s3://coiled-datasets/dask-book/nyc-tlc/2009\",\n",
    "    storage_options={\"anon\": True},\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba566750-2017-44ba-9595-abba9fb5353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd3127-3932-4f1c-a6b9-462aa5592af5",
   "metadata": {},
   "source": [
    "<img src=\"images/nyc-taxi-scatter.png\" align=\"right\" width=\"40%\">\n",
    "\n",
    "### Plotting large scale data is hard\n",
    "\n",
    "Let's say we wanted to get a map of where taxi's dropped off passengers.  In principle we'd want something like the following:\n",
    "\n",
    "```python\n",
    "df.sample(frac=0.001).compute().plot(\n",
    "    x=\"pickup_longitude\", \n",
    "    y=\"pickup_latitude\", \n",
    "    kind=\"scatter\",\n",
    ")\n",
    "```\n",
    "\n",
    "Even at 0.1% downsampling this is still just a big blob of blue.\n",
    "\n",
    "We can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17d6f6-69f2-4b61-b7e7-996cb126c55e",
   "metadata": {},
   "source": [
    "### Datashader for large scale visualization\n",
    "\n",
    "[Datashader](https://datashader.org/) is a Python library designed to visualize large datasets.  It also happens to build on Dask.  It renders large volumes of data with better design.\n",
    "\n",
    "We won't go into how Datashader works in this tutorial (there are excellent resources online) for us it's just a tool to show us that we're processing our data quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8350c94-0593-4659-a7d5-d5b892e67ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader\n",
    "from datashader import transfer_functions as tf\n",
    "from datashader.colors import Hot\n",
    "import holoviews as hv\n",
    "\n",
    "def render(df, x_range=(-74.1, -73.7), y_range=(40.6, 40.9)):\n",
    "    # Plot\n",
    "    canvas = datashader.Canvas(\n",
    "        x_range=x_range,\n",
    "        y_range=y_range,\n",
    "    )\n",
    "    agg = canvas.points(\n",
    "        source=df, \n",
    "        x=\"dropoff_longitude\", \n",
    "        y=\"dropoff_latitude\", \n",
    "        agg=datashader.count(\"passenger_count\"),\n",
    "    )\n",
    "    return datashader.transfer_functions.shade(agg, cmap=Hot, how=\"eq_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd2893-f9b8-4f69-bfca-8a98a930e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "render(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e227c09-16e7-4508-bd45-a7042f2ca05f",
   "metadata": {},
   "source": [
    "## Let's Speed This Up\n",
    "That works...technically. But it's painfully slow to render. How can we speed this up?\n",
    "\n",
    "One of the time-consuming tasks here is fetching the data from S3. We can `.persist()` the dataframe into our cluster memory before we render the interactive plot. That should speed things up a bit.\n",
    "\n",
    "Persist the dataframe in memory like we did at the end of the last notebook.\n",
    "-  How long does this take?\n",
    "-  How long does it take to render subsequent images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bcfce-9c2a-475e-93df-a42452c5f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: persist dataframe in memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acda2e-a2fc-49fc-8b0b-1771f823ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "render(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22605807-012c-4786-8e14-42ed2bc6bd9c",
   "metadata": {},
   "source": [
    "## Interact\n",
    "\n",
    "Now that we have this running at decent interactive speeds, let's switch Datashader to interactive mode.\n",
    "\n",
    "Warning!  There is some spurious data, so you will likely have to zoom in quite a bit.  \n",
    "\n",
    "Alternatively, if you wanted to clean things up a bit, you could use pandas syntax to filter out rows outside the region where dropoff_longitude between (-74.1, -73.7) and dropoff_latitude is between (40.6, 40.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f00a3-2092-44e0-a46a-bb09e40f6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.dask\n",
    "\n",
    "def interact(df):\n",
    "    return df.hvplot.scatter(\n",
    "        x=\"dropoff_longitude\", \n",
    "        y=\"dropoff_latitude\", \n",
    "        aggregator=datashader.count(\"passenger_count\"), \n",
    "        datashade=True, cnorm=\"eq_hist\", cmap=Hot,\n",
    "        width=600, \n",
    "        height=400,\n",
    "    )\n",
    "\n",
    "interact(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08835c68-23cb-4dcc-b293-4c0e030220e1",
   "metadata": {},
   "source": [
    "Play around and look at interesting bits.  \n",
    "\n",
    "Can you spot the three airports from our last exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8d3c2-56c1-4d71-bd86-a90c7a647895",
   "metadata": {},
   "source": [
    "## More Data\n",
    "\n",
    "As we interact with our data and zoom in we find that we run out of data.  Fortunately, we have more. \n",
    "\n",
    "So far we've looked at the data for one year, 2009.  NYC-TLC published fine-grained location information for five years from 2009-2013.  This data is stored in parquet format in an S3 bucket at this location:\n",
    "\n",
    "```python\n",
    "\"s3://coiled-datasets/dask-book/nyc-tlc/2009-2013/\"\n",
    "```\n",
    "\n",
    "Read this data into a new Dask dataframe using the `dd.read_parquet` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259cb552-5667-407d-9c73-cd53aaaf94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet(\n",
    "    \"s3://coiled-datasets/dask-book/nyc-tlc/2009-2013/\",\n",
    "    storage_options={\"anon\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44939ad-8bdb-4041-a244-637c9945833d",
   "metadata": {},
   "source": [
    "- How many rides does it represent?\n",
    "- How much money did passengers pay roughly?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763e8cf-3698-4d45-aa4e-e2027512c38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300cd41-997a-491d-9cd1-1fe4cb256ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5967b60e-d825-4066-889f-79d972090b83",
   "metadata": {},
   "source": [
    "Let's visualize this entire dataset as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1b613-77e3-4723-a666-5a81db186dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "render(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6281ba8-0b44-4c19-ae5b-4fef33bebbbd",
   "metadata": {},
   "source": [
    "## Persist, observe dashboard\n",
    "\n",
    "Let's do the same trick that we did last time and persist this data in memory to make it faster.\n",
    "\n",
    "What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0289f24-6284-4b09-818b-a1fbc533874d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b73f65f-8a1c-478d-a0bd-53cd3e269fd5",
   "metadata": {},
   "source": [
    "Watch the dashboard for a couple minutes.  \n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5228be6-f226-48ff-a3f8-cdbd2d950ac3",
   "metadata": {},
   "source": [
    "## Reduce dataset size in memory\n",
    "\n",
    "Our data is too big for our cluster.  We have two options:\n",
    "\n",
    "1.  Get a bigger cluster\n",
    "2.  Reduce the size of our data\n",
    "\n",
    "We could get a bigger cluster with the command `cluster.scale(...)`, but that's wasteful.  Instead, let's be efficient and reduce the size of our data.  There are three things that we can consider:\n",
    "\n",
    "1.  Use better data types like `\"string[pyarrow]\"` for object dtypes, more compact floats and ints, and categoricals (just using `string[pyarrow]` is usually very effective).\n",
    "2.  Eliminate all of the columns that we don't need\n",
    "3.  Sampling (but we want all of our data)\n",
    "\n",
    "Play around, see if you can get a configuration that fits nicely into memory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7417cca3-a027-4e59-b566-4bbcfdca2773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f78f2-e0db-4a22-aaa2-213d14b299d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c190954-3072-406b-a9b7-aa86c7d51f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf56de-ebea-41e2-9b08-2869875c0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd29b7-807a-4d0e-be3b-e69ef2777040",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "render(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e227e96-9a84-4710-be43-d272e8805331",
   "metadata": {},
   "source": [
    "## Reduce number of partitions\n",
    "\n",
    "Our data was originally in nicely sized partitions of 100-500 MiB.  This is a good size of data to play with.  Now that we have become more efficient our partitions are a lot smaller.  We still have thousands of them.  We can reduce overhead by consolidating our dataframe into fewer larger partitions, again aiming for that 100-500 MiB number.\n",
    "\n",
    "Let's do this work in two parts:\n",
    "\n",
    "1.  Investigate the current chunk size.  There are a few methods:\n",
    "    1.  Bring a single partition locally with the `.partitions` accessor (which you'll have to look up), bring it local with the compute call, and then use the `pandas` `.memory_usage()` method.\n",
    "    2.  Map the `pandas.DataFrame.memory_usage` method across all of your partitions with the `map_partitions` method (which you'll have to look up)\n",
    "    3.  Navigate to the Dask dashboard (address at `client.dashboard_link`), go to the Info pages tab in the upper right, navigate to a worker and then to a task and see information about that task to see how large Dask thinks it is.  Look at a few appropriate tasks.\n",
    "    \n",
    "2.  Compare this to the current number of partitions (use the `.npartitions` attribute) and determine a good number of output partitions (the partition size should be between 100-500 MiB each).  Repartition the dataframe to this number and persist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a25bec-75f9-4030-8662-e4741be25842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2bacc2-37d2-4c33-b4d4-a0f7b06042db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9daf730-21f2-4ee4-93b6-0c6b87bb02d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f49658-d65b-4dcb-9a79-1d7c90eae068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(npartitions=...).persist()\n",
    "wait(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073b293-3557-464e-959b-6d3fb5ed4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "render(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc29fd50-f9e1-41e4-9625-681da662ea60",
   "metadata": {},
   "source": [
    "You should be able to get this down to 1-3s.  Congratulations.  You now deserve to interact with your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43591401-a4f5-438d-93ef-ac3d61e8f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fc653-34ed-4c77-a4a2-4bdd8337e523",
   "metadata": {},
   "source": [
    "## Include pickup and dropoff locations\n",
    "\n",
    "So far we've only been looking at one of these two datasets.  Now we'll look at both together. \n",
    "\n",
    "We now take all of our lessons learned to set this up for interactive scaling.  \n",
    "\n",
    "We'll be visualizing and interacting with 1+B points now.\n",
    "\n",
    "You don't need to do anything, just execute these cells and play at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea44db2-5f05-4941-97af-14bd6850a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in one year of NYC Taxi data\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet(\n",
    "    \"s3://coiled-datasets/dask-book/nyc-tlc/2009\",\n",
    "    storage_options={\"anon\": True},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c361e19-7e71-498a-964a-3d1addcf7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"dropoff_longitude\", \"dropoff_latitude\", \"pickup_longitude\", \"pickup_latitude\"]]\n",
    "\n",
    "# clean data\n",
    "df = df.loc[\n",
    "    (df.dropoff_longitude > -74.1) & (df.dropoff_longitude < -73.7) & \n",
    "    (df.dropoff_latitude > 40.6) & (df.dropoff_latitude < 40.9) &\n",
    "    (df.pickup_longitude > -74.1) & (df.pickup_longitude < -73.7) &\n",
    "    (df.pickup_latitude > 40.6) & (df.pickup_latitude < 40.9)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f9713-1787-4562-94c7-31f0ead12d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_dropoff = df[[\"dropoff_longitude\", \"dropoff_latitude\"]]\n",
    "df_dropoff[\"journey_type\"] = \"dropoff\"\n",
    "df_dropoff = df_dropoff.rename(columns={'dropoff_longitude': 'long', 'dropoff_latitude': 'lat'})\n",
    "df_pickup = df[[\"pickup_longitude\", \"pickup_latitude\"]]\n",
    "df_pickup[\"journey_type\"] = \"pickup\"\n",
    "df_pickup = df_pickup.rename(columns={'pickup_longitude': 'long', 'pickup_latitude': 'lat'})\n",
    "df = dd.concat([df_dropoff, df_pickup])\n",
    "\n",
    "pickup_dropoff = pd.CategoricalDtype(categories=[\"pickup\", \"dropoff\"])\n",
    "df = df.astype({\"journey_type\": pickup_dropoff})\n",
    "\n",
    "df = df.repartition(partition_size=\"256Mib\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5896c8-c9d7-4833-b4a1-fffa886d51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader\n",
    "import hvplot.dask\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "color_key = {'pickup': \"#e41a1c\", 'dropoff': \"#377eb8\"}\n",
    "\n",
    "df.hvplot.scatter(\n",
    "    x=\"long\", \n",
    "    y=\"lat\", \n",
    "    aggregator=datashader.by(\"journey_type\"), \n",
    "    datashade=True, \n",
    "    cnorm=\"eq_hist\",\n",
    "    width=700,\n",
    "    aspect=1.33, \n",
    "    color_key=color_key\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
