{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c22a243-93a0-473c-95ba-a718baa2edf0",
   "metadata": {},
   "source": [
    "# Scaling, Performance, and Memory\n",
    "\n",
    "In this notebook we will work with a multi-machine cluster operating in the cloud.  We will do performance tuning on a workflow that enables interactie visualization, and learn about how to measure and improve performance in a distributed context.  We'll make some pretty images too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001ca7e-4d5d-4d90-b456-7c21429d8060",
   "metadata": {},
   "source": [
    "## Request Dask Cluster\n",
    "\n",
    "There are many services to create Dask clusters in the cloud.  Today we'll use Coiled.\n",
    "\n",
    "This should take a couple minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e571e7-26cc-4aed-b946-3c1e7fa412e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: !coiled login --token TOKEN --account coiled-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4c47f-0556-47e2-b9d1-c80b54a59041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    n_workers=10,\n",
    "    package_sync=True,\n",
    "    scheduler_port=443\n",
    ")\n",
    "\n",
    "from dask.distributed import Client, wait\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea17dca-18fc-433b-89e5-748faa599ba7",
   "metadata": {},
   "source": [
    "## Large Scale GIS Visualization\n",
    "\n",
    "For our application we'll visualize the taxi pickup locations in the classic NYC Taxi dataset.  \n",
    "\n",
    "This data is available to us in Parquet format on S3.  Let's take a brief look at the first few rows and see how many rows we have in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc4687b-2eca-4e5f-8799-94113e02959c",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33582164-1ca1-4685-beb0-03502334baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in one year of NYC Taxi data\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet(\n",
    "    \"s3://coiled-datasets/dask-book/nyc-tlc/2009\",\n",
    "    storage_options={\"anon\": True},\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba566750-2017-44ba-9595-abba9fb5353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd3127-3932-4f1c-a6b9-462aa5592af5",
   "metadata": {},
   "source": [
    "<img src=\"images/nyc-taxi-scatter.png\" align=\"right\" width=\"40%\">\n",
    "\n",
    "### Plotting large scale data is hard\n",
    "\n",
    "Let's say we wanted to get a map of where taxi's dropped off passengers.  In principle we'd want something like the following:\n",
    "\n",
    "```python\n",
    "df.sample(frac=0.001).compute().plot(\n",
    "    x=\"pickup_longitude\", \n",
    "    y=\"pickup_latitude\", \n",
    "    kind=\"scatter\",\n",
    ")\n",
    "```\n",
    "\n",
    "Even at 0.1% downsampling this is still just a big blob of blue.\n",
    "\n",
    "We can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17d6f6-69f2-4b61-b7e7-996cb126c55e",
   "metadata": {},
   "source": [
    "### Datashader for large scale visualization\n",
    "\n",
    "[Datashader](https://datashader.org/) is a Python library designed to visualize large datasets.  It also happens to build on Dask.  It renders large volumes of data with better design.\n",
    "\n",
    "We won't go into how Datashader works in this tutorial (there are excellent resources online) for us it's just a tool to show us that we're processing our data quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8350c94-0593-4659-a7d5-d5b892e67ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader\n",
    "from datashader import transfer_functions as tf\n",
    "from datashader.colors import Hot\n",
    "import holoviews as hv\n",
    "\n",
    "def render(df, x_range=(-74.1, -73.7), y_range=(40.6, 40.9)):\n",
    "    # Plot\n",
    "    canvas = datashader.Canvas(\n",
    "        x_range=x_range,\n",
    "        y_range=y_range,\n",
    "    )\n",
    "    agg = canvas.points(\n",
    "        source=df, \n",
    "        x=\"dropoff_longitude\", \n",
    "        y=\"dropoff_latitude\", \n",
    "        agg=datashader.count(\"passenger_count\"),\n",
    "    )\n",
    "    return datashader.transfer_functions.shade(agg, cmap=Hot, how=\"eq_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd2893-f9b8-4f69-bfca-8a98a930e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "render(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e227c09-16e7-4508-bd45-a7042f2ca05f",
   "metadata": {},
   "source": [
    "## Let's Speed This Up\n",
    "That works...technically. But it's painfully slow to render. How can we speed this up?\n",
    "\n",
    "One of the time-consuming tasks here is fetching the data from S3. We can `.persist()` the dataframe into our cluster memory before we render the interactive plot. That should speed things up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9595f731-0183-4eba-ab18-d843d0f50008",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()\n",
    "wait(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a26848-172a-4dd1-b767-dc349e3cef07",
   "metadata": {},
   "source": [
    "Let's try again.  It ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acda2e-a2fc-49fc-8b0b-1771f823ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "render(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116a581-e3bb-4d65-9a1f-3fbfda57f4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22605807-012c-4786-8e14-42ed2bc6bd9c",
   "metadata": {},
   "source": [
    "## Interact\n",
    "\n",
    "Now that we have this running at decent interactive speeds, let's switch Datashader to interactive mode.\n",
    "\n",
    "Warning!  There is some spurious data, so you will likely have to zoom in quite a bit.  \n",
    "\n",
    "TODO: \n",
    "1.  Fix aspect ratio\n",
    "2.  Start user nearer NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f00a3-2092-44e0-a46a-bb09e40f6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.dask\n",
    "\n",
    "def interact(df):\n",
    "    return df.hvplot.scatter(\n",
    "        x=\"dropoff_longitude\", \n",
    "        y=\"dropoff_latitude\", \n",
    "        aggregator=datashader.count(\"passenger_count\"), \n",
    "        datashade=True, cnorm=\"eq_hist\", cmap=Hot,\n",
    "        width=600, \n",
    "        height=400,\n",
    "    )\n",
    "\n",
    "interact(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08835c68-23cb-4dcc-b293-4c0e030220e1",
   "metadata": {},
   "source": [
    "Play around and look at interesting bits.  \n",
    "\n",
    "Can you spot the three airports from our last exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8d3c2-56c1-4d71-bd86-a90c7a647895",
   "metadata": {},
   "source": [
    "## More Data\n",
    "\n",
    "As we interact with our data and zoom in we find that we run out of data.  Fortunately, we have more. \n",
    "\n",
    "So far we've looked at the data for one year, 2009.  NYC-TLC published fine-grained location information for five years from 2009-2013.  This data is stored in parquet format in an S3 bucket at this location:\n",
    "\n",
    "```python\n",
    "\"s3://coiled-datasets/dask-book/nyc-tlc/2009-2013/\"\n",
    "```\n",
    "\n",
    "Read this data into a new Dask dataframe using the `dd.read_parquet` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259cb552-5667-407d-9c73-cd53aaaf94d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a44939ad-8bdb-4041-a244-637c9945833d",
   "metadata": {},
   "source": [
    "- How many rides does it represent?\n",
    "- How much money did passengers pay roughly?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763e8cf-3698-4d45-aa4e-e2027512c38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300cd41-997a-491d-9cd1-1fe4cb256ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5967b60e-d825-4066-889f-79d972090b83",
   "metadata": {},
   "source": [
    "Let's visualize this entire dataset as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1b613-77e3-4723-a666-5a81db186dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "render(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6281ba8-0b44-4c19-ae5b-4fef33bebbbd",
   "metadata": {},
   "source": [
    "## Persist, observe dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0289f24-6284-4b09-818b-a1fbc533874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make exercise\n",
    "\n",
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73f65f-8a1c-478d-a0bd-53cd3e269fd5",
   "metadata": {},
   "source": [
    "Watch the dashboard for a couple minutes.  \n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5228be6-f226-48ff-a3f8-cdbd2d950ac3",
   "metadata": {},
   "source": [
    "## Reduce dataset size in memory\n",
    "\n",
    "Our data is too big for our cluster.  We have two options:\n",
    "\n",
    "1.  Get a bigger cluster\n",
    "2.  Reduce the size of our data\n",
    "\n",
    "Let's be efficient, and reduce the size of our data.  There are three things that we can consider:\n",
    "\n",
    "1.  Use better data types like `\"string[pyarrow]\"` for object dtypes, more compact floats and ints, and categoricals\n",
    "2.  Eliminate all of the columns that we don't need\n",
    "3.  Sampling\n",
    "\n",
    "Play around, see if you can get a configuration that fits nicely into memory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae4c770-b668-4c58-b2b3-9584fecfb955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make exercise\n",
    "\n",
    "df = df[[\"dropoff_latitude\", \"dropoff_longitude\", \"passenger_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18f78f2-e0db-4a22-aaa2-213d14b299d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c190954-3072-406b-a9b7-aa86c7d51f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bf56de-ebea-41e2-9b08-2869875c0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd29b7-807a-4d0e-be3b-e69ef2777040",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "render(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e227e96-9a84-4710-be43-d272e8805331",
   "metadata": {},
   "source": [
    "## Reduce number of partitions\n",
    "\n",
    "The actual Dask work here was fast, but it took a long time for Dask to get going.  \n",
    "\n",
    "We suspect that this is because it was hard for Dask to upload the plan/graph across WiFi to the cluster.  \n",
    "\n",
    "Let's reduce the amount of work that Dask has to do here by reducing the number of partitions.\n",
    "\n",
    "### Good partition size\n",
    "\n",
    "Partitions should generally be between 100-500 MiB, depending on the size of the worker you have, the dataset you're working with, and the computation that you're doing.\n",
    "\n",
    "Your data started out this way, but after you reduced it it's probably a lot smaller.  Use `df.repartition` to collect your same data into fewer partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34f458-9a65-4d5a-8acc-96e2006d3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make exercise\n",
    "\n",
    "df = df.repartition(partition_size=\"256 MiB\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159f5e11-d095-4433-bb38-f5d10f666737",
   "metadata": {},
   "outputs": [],
   "source": [
    "render(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43591401-a4f5-438d-93ef-ac3d61e8f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d6a03-0685-4179-a859-9f407f0dbadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c50da-bd78-4009-86fc-ffb1cf4de244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "603fc653-34ed-4c77-a4a2-4bdd8337e523",
   "metadata": {},
   "source": [
    "## Include pickup and dropoff locations\n",
    "\n",
    "So far we've only been looking at one of these two datasets.  Now we'll look at both together. \n",
    "\n",
    "We now take all of our lessons learned to set this up for interactive scaling.  \n",
    "\n",
    "We'll be visualizing and interacting with 1+B points now.\n",
    "\n",
    "You don't need to do anything, just execute these cells and play at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea44db2-5f05-4941-97af-14bd6850a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in one year of NYC Taxi data\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet(\n",
    "    \"s3://coiled-datasets/dask-book/nyc-tlc/2009\",\n",
    "    storage_options={\"anon\": True},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c361e19-7e71-498a-964a-3d1addcf7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"dropoff_longitude\", \"dropoff_latitude\", \"pickup_longitude\", \"pickup_latitude\"]]\n",
    "\n",
    "# clean data\n",
    "df = df.loc[\n",
    "    (df.dropoff_longitude > -74.1) & (df.dropoff_longitude < -73.7) & \n",
    "    (df.dropoff_latitude > 40.6) & (df.dropoff_latitude < 40.9) &\n",
    "    (df.pickup_longitude > -74.1) & (df.pickup_longitude < -73.7) &\n",
    "    (df.pickup_latitude > 40.6) & (df.pickup_latitude < 40.9)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f9713-1787-4562-94c7-31f0ead12d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_dropoff = df[[\"dropoff_longitude\", \"dropoff_latitude\"]]\n",
    "df_dropoff[\"journey_type\"] = \"dropoff\"\n",
    "df_dropoff = df_dropoff.rename(columns={'dropoff_longitude': 'long', 'dropoff_latitude': 'lat'})\n",
    "df_pickup = df[[\"pickup_longitude\", \"pickup_latitude\"]]\n",
    "df_pickup[\"journey_type\"] = \"pickup\"\n",
    "df_pickup = df_pickup.rename(columns={'pickup_longitude': 'long', 'pickup_latitude': 'lat'})\n",
    "df = dd.concat([df_dropoff, df_pickup])\n",
    "\n",
    "pickup_dropoff = pd.CategoricalDtype(categories=[\"pickup\", \"dropoff\"])\n",
    "df = df.astype({\"journey_type\": pickup_dropoff})\n",
    "\n",
    "df = df.repartition(partition_size=\"256Mib\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5896c8-c9d7-4833-b4a1-fffa886d51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader\n",
    "import hvplot.dask\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "color_key = {'pickup': \"#e41a1c\", 'dropoff': \"#377eb8\"}\n",
    "\n",
    "df_plot.hvplot.scatter(\n",
    "    x=\"long\", \n",
    "    y=\"lat\", \n",
    "    aggregator=datashader.by(\"journey_type\"), \n",
    "    datashade=True, \n",
    "    cnorm=\"eq_hist\",\n",
    "    width=700,\n",
    "    aspect=1.33, \n",
    "    color_key=color_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba068c13-02d3-40d3-ba96-e2d43627bccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
