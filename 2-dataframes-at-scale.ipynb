{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfeab872-7758-4792-b75b-18569f84db08",
   "metadata": {},
   "source": [
    "DataFrames at Scale\n",
    "===================\n",
    "\n",
    "*Playing with memory and performance*\n",
    "\n",
    "**Technical Goal:** visualize large volumes of NYC Taxi data quickly.\n",
    "\n",
    "**Pedagogical Goal:** Learn about managing memory use, how to interpret the dashboard, and how to improve performance on dataframe workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0092df3-61dc-45a1-bbf3-956d62608332",
   "metadata": {},
   "source": [
    "Visualize NYC Taxi Data\n",
    "-----------------------\n",
    "\n",
    "We can visualize the NYC Taxi data using Pandas and Datashader, a plotting library designed for large datasets.\n",
    "\n",
    "*Datashader Motivation: if you do a scatter-plot of a billion points things break.  It's slow and it also looks like just a solid blob of points.  Datashader does more intelligent rendering.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1a1fee-8c4b-4328-9823-e6662ac39d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(one-file-of-nyc-taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8734196-7491-4718-807c-5db4062d75b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader\n",
    "\n",
    "datashader.render_nice_image(df, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec5bca-ccea-4305-adb3-55db7eb780e1",
   "metadata": {},
   "source": [
    "## Exercise: Explore using Datashader pan/zoom\n",
    "\n",
    "Datashader allows you to pan and zoom and explore the data.  It re-renders whenever you do so.  Explore the dataset now for a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a186eb44-cc5a-4a5c-ae1c-09ea385d61df",
   "metadata": {},
   "source": [
    "## Create Dask Cluster in the cloud\n",
    "\n",
    "For this notebook we're going to use a Dask cluster running near the data on the cloud.  It should take a couple of minutes for us to get these machines with all of the right software installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7737172-137f-4ad4-bda4-d0d9119af77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    package_sync=True,\n",
    "    backend_options={\"region\": \"us-east-1\"},\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e759742-0992-41dc-bea4-68cf7e68aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster)  # Point Dask to use this cluster for all operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3682fe4b-cab3-4744-923e-9cea466f9ff3",
   "metadata": {},
   "source": [
    "## Exercise: Load more data\n",
    "\n",
    "Datashader knows about Dask and will execute in parallel on very large datasets if given a Dask dataframe. \n",
    "\n",
    "Rewrite the pandas code above to use Dask dataframe and then use Datashader to render the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254e9cb-dadf-4bf7-ab7c-8a670b54ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet(\"s3://nyc-tlc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ef608-8bf3-4c5a-b42d-9d85e31b186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: datashader code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4026a8cb-9a65-4960-b7c6-6c08496fc953",
   "metadata": {},
   "source": [
    "Hopefully this looks impressive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c7792-d292-4870-bc07-4553c831fab6",
   "metadata": {},
   "source": [
    "## Exercise: View the Dashboard\n",
    "\n",
    "How do we make this faster?  The secret to performance is measurement.  Fortunately Dask is making all sorts of measurements.  These are available to us through the Dask Dashboard.\n",
    "\n",
    "Run this again, but this time observe the Dask dashboard.  What do you observe?\n",
    "\n",
    "If you want to look through additional dashboard plots consider trying:\n",
    "\n",
    "-  Task Stream\n",
    "-  Progress\n",
    "-  Workers Memory\n",
    "-  Profile\n",
    "-  Workers Bandwidth\n",
    "\n",
    "We'll have a conversation about this.  Write down any observations that you'd like to share, especially about what might be slowing us down.\n",
    "\n",
    "\n",
    "\n",
    "How fast is our current response time whenever we pan/zoom?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b651c1-1d42-449c-98d6-f6834c225278",
   "metadata": {},
   "source": [
    "## Persist data in memory to improve performance\n",
    "\n",
    "Mostly we're slowed down by reading Parquet Data from S3.  Each time we pan/zoom Dask has to read the entire dataset from S3 again.  This is slow.\n",
    "\n",
    "We can avoid this if we `persist` the data in memory.  We do this below.  \n",
    "\n",
    "Watch the dashboard as we run this command.  What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff5217-a4c3-49fc-bf16-df8cde1ecdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a90c9-d883-4e98-96dd-29495bb49c7f",
   "metadata": {},
   "source": [
    "## Too much Data\n",
    "\n",
    "-  How much memory does our cluster have?  You can find this out in a few ways:\n",
    "    -   The cluster memory dashboard plot\n",
    "    -   The client `repr`\n",
    "    -   client.scheduler_info()\n",
    "\n",
    "-  How much data does our dataset take in memory?  You can find this out in a few ways:\n",
    "    -   The cluster memory dashboard plot\n",
    "    -   `df.memory_usage(deep=True).compute()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc0087-ec23-4c39-81f1-89f9482fa7ac",
   "metadata": {},
   "source": [
    "## Exercise: reduce memory use by sampling\n",
    "\n",
    "Find a method in the [dask.dataframe API](https://docs.dask.org/en/stable/dataframe-api.html) to sample down the dataset an appropriate amount. \n",
    "\n",
    "Persist that dataset and overwrite the previous one.  Is the data small enough to pan and zoom smoothly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543639dc-7cf7-4276-a602-d8ee3f15e2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98fba3-0098-4ddb-a8f8-eeb58750f83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "857f80e1-6890-4315-9cf2-cf724ef6dd45",
   "metadata": {},
   "source": [
    "## Exercise: reduce memory use by column and dtype refinement\n",
    "\n",
    "We're probably storing lots of data in memory that we don't need.  Try removing columns and casting dtypes to smaller forms to reduce memory. \n",
    "\n",
    "How slim can you make the dataset while still getting all of the rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b104e86-e750-4bac-b204-6b8d9a3b5cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ae0e1-4c25-4892-8f9c-2dc0f4c0b991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7e152a5-9e63-4499-879d-cbab22d8b784",
   "metadata": {},
   "source": [
    "## Exercise: scale\n",
    "\n",
    "We're probably good at this point, but another way to handle larger datasets is to scale your cluster.  \n",
    "\n",
    "Inspect the `cluster.scale` method.  Then scale up your cluster to twice as many machines.  How long does this take?\n",
    "\n",
    "*Tip: use the `Cluster Map` or `Workers` dashboard plot to view the number of workers live*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912c64c-0565-44e6-bac0-75e39411a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566117a1-66d0-432a-9ff7-aede608baec0",
   "metadata": {},
   "source": [
    "Run your computation again.  Does this make things go any faster?  \n",
    "\n",
    "Why or why not do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6975d-9f0b-45c3-b491-5886012b89f4",
   "metadata": {},
   "source": [
    "## Shut down your cluster\n",
    "\n",
    "This is polite to do.  It'll shut off in 20 minutes regardless though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2087874-dc9c-4a87-9dcd-1e7da9c34964",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:play]",
   "language": "python",
   "name": "conda-env-play-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
